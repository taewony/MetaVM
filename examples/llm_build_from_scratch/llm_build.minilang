# 이 코드는 데이터 파이프라인 및 ML 워크플로우에 특화된 minilang DSL의 개념을 설명하기 위한 예시입니다.
# 실제 실행 가능한 코드는 minilang MetaVM 및 관련 라이브러리 구현에 따라 달라질 수 있습니다.

# @intent 데코레이터: 이 Pipeline의 목적을 설명합니다.
@intent("CPU/GPU 통합 실행 환경에서 LLM build from scratch")

# @parallel 데코레이터: 실행 환경의 병렬 처리 설정을 선언합니다.
# type="hybrid": CPU와 GPU를 혼합 사용
# cpu_workers: CPU 작업자 수
# gpu_streams: GPU 스트림 수 (동시 실행 가능한 GPU 작업 단위)
@parallel(type="hybrid", cpu_workers=8, gpu_streams=4)

# @execution_context: 전역 실행 환경 설정을 선언합니다.
@execution_context(
    mixed_precision="bfloat16", # 혼합 정밀도 사용 (bfloat16)
    gradient_accumulation=4, # 그래디언트 누적 스텝 수
    profiler="nsight" # 사용할 프로파일러 (예시)
)

# Entity: 데이터 준비 관련 Block들을 묶는 논리적 단위 (GitHub 디렉토리 매핑)
entity data_preparation:

    # Block: 원본 텍스트 파일을 읽고 토큰화하는 기능
    block DataPipeline:
        # inputs: 이 Block이 필요로 하는 외부 입력 (파일 경로 패턴)
        inputs:
            text_files_pattern: str # 예: "data/*.txt"

        # outputs: 이 Block이 생성하는 결과 (토큰화된 데이터 배치)
        outputs:
            tokenized_batches: torch.Tensor # PyTorch Tensor 타입 출력

        # config: 이 Block의 동작 방식 설정
        config:
            vocab_size: int = 50000 # 어휘 크기
            batch_size: int = 256 # 배치 크기
            prefetch_batches: int = 10 # 미리 가져올 배치 수
            # Pandas를 사용하는 경우:
            # use_pandas_for_loading: bool = False # 데이터 로딩 시 Pandas 사용 여부

        # pipeline: 이 Block 내부의 하위 작업 흐름 정의 (선언적)
        pipeline:
            # load_text: 입력 패턴에 해당하는 텍스트 파일들을 로딩하는 내장/외부 Block 호출
            # 이 Block은 내부적으로 Python의 파일 I/O 또는 Pandas 등을 사용할 수 있습니다.
            let corpus = load_text(inputs.text_files_pattern) # 결과는 문자열 또는 Pandas Series/DataFrame 가능

            # BPETokenizer: BPE 토크나이저 Block 인스턴스 생성 및 설정 적용
            # 이 Block은 내부적으로 Hugging Face tokenizers 라이브러리 등을 사용할 수 있습니다.
            let tokenizer = BPETokenizer(config.vocab_size)

            # encode: 토크나이저 Block의 encode 함수 호출
            # 결과는 NumPy ndarray 또는 Torch Tensor 가능
            let encoded_corpus = tokenizer.encode(corpus)

            # 데이터 처리 체인 (메서드 체이닝 형태의 DSL 표현)
            # .to_gpu(): 데이터를 GPU 메모리로 이동 (PyTorch Tensor에 적용 가능)
            # .shuffle(): 데이터 셔플링 (NumPy 또는 PyTorch 연산 사용)
            # .batch(): 배치 단위로 묶기 (커스텀 로직 또는 라이브러리 함수 사용)
            # .prefetch(): 다음 배치를 미리 로딩 (minilang MetaVM 기능 또는 라이브러리 활용)
            let batches = encoded_corpus.to_gpu() >> shuffle() >> batch(config.batch_size) >> prefetch(config.prefetch_batches)

            # 최종 출력을 Pipeline의 outputs에 연결
            >> outputs.tokenized_batches = batches

# Entity: 모델 아키텍처 정의 관련 Block들을 묶는 논리적 단위
entity model_architecture:

    # Block: 트랜스포머 모델 아키텍처 정의
    block Transformer:
        # inputs: 모델의 입력 데이터 텐서
        inputs:
            input_tensor: torch.Tensor

        # outputs: 모델의 출력 텐서 (logits 등)
        outputs:
            output_tensor: torch.Tensor

        # config: 모델 하이퍼파라미터 설정
        config:
            num_layers: int = 12
            hidden_dim: int = 768
            num_heads: int = 12
            # cuTILING 관련 설정
            attention_tile_size: int = 128

        # @kernel: cuTILING/CUDA 등을 사용하여 커스텀 커널을 정의함을 선언
        # cuda=True: CUDA 커널임을 명시
        # tile_size: cuTILING 관련 타일 크기 설정 (Block config에서 가져옴)
        @kernel(cuda=True, tile_size=config.attention_tile_size)
        # attention: 어텐션 메커니즘을 구현하는 함수 (커널로 실행될 부분)
        # ml.einsum, ml.softmax 등은 minilang이 제공하거나 래핑하는 연산 (PyTorch 또는 cuTILING 연산 사용)
        def attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
            # cuTILE 기반 커스텀 커널 로직 (DSL 내에서 저수준 연산 표현)
            # 내부적으로 cuTILING Python API를 호출하여 구현될 수 있습니다.
            scores = ml.einsum("bhid,bhjd->bhij", Q, K) # 배치, 헤드, 시퀀스 길이, 차원
            attention_weights = ml.softmax(scores, dim=-1)
            output = ml.einsum("bhin,bhnj->bhij", attention_weights, V)
            return output

        # forward: 모델의 순방향 계산을 정의하는 함수 (Block의 주요 실행 로직)
        # 이 함수는 Block 인스턴스가 호출될 때 실행됩니다.
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            # 모델 레이어 반복 (선언적 루프 표현)
            for i in range(config.num_layers):
                # LayerNorm, MLP 등은 다른 Block을 호출하거나 내장 연산을 사용 (PyTorch LayerNorm, 선형 변환 등)
                # self.attention(): Block 내부에 정의된 커널 함수 호출
                attn_output = self.attention(x, x, x) # Self-attention 예시
                x = ml.layer_norm(x + attn_output) # Add & Norm

                # MLP Block/연산 호출 (내부적으로 PyTorch Linear 레이어 등을 사용)
                mlp_output = ml.mlp(x, hidden_dim=config.hidden_dim)
                x = ml.layer_norm(x + mlp_output) # Add & Norm

            return x >> outputs.output_tensor # 최종 결과를 출력에 연결

# Entity: 학습 관련 Block들을 묶는 논리적 단위
entity training:

    # Block: LLM 학습 루프 정의
    block Training:
        # requires: 이 Block이 의존하는 다른 Block의 출력 또는 정의
        requires:
            data: data_preparation.DataPipeline.tokenized_batches # DataPipeline의 출력에 의존
            model_def: model_architecture.Transformer # Transformer Block 정의 자체에 의존

        # config: 학습 관련 설정
        config:
            epochs: int = 100 # 학습 에폭 수
            learning_rate: float = 3e-4 # 학습률
            save_threshold_loss: float = 1.0 # 모델 저장 손실 임계값
            # 옵티마이저 관련 설정
            optimizer_type: str = "Adam" # 사용할 옵티마이저 타입

        # pipeline: 학습 루프 작업 흐름 정의
        pipeline:
            # model_def를 사용하여 Transformer Block의 인스턴스를 생성하고 GPU로 이동
            let model = requires.model_def().to("cuda:0")

            # 옵티마이저 Block/함수 호출 (config에 따라 동적으로 선택)
            let optimizer = ml.Optimizer(model.parameters(), type=config.optimizer_type, lr=config.learning_rate)

            # 학습 에폭 루프 (선언적 반복)
            for epoch in range(config.epochs):
                # DataPipeline Block에서 다음 배치 데이터를 가져옴
                let data_batch = requires.data.next_batch()

                # 모델 순방향 계산 (Transformer Block의 forward 함수 호출)
                let logits = model(data_batch.inputs) # data_batch에서 입력 데이터 추출

                # 손실 계산 (ml.cross_entropy 내장/외부 Block 호출 - PyTorch 손실 함수 사용)
                let loss = ml.cross_entropy(logits, data_batch.labels) # data_batch에서 레이블 추출

                # <<backward>>: 역전파 연산 선언 (PyTorch autograd 등 활용)
                <<backward>> loss

                # optimizer.step(): 옵티마이저 스텝 (PyTorch 옵티마이저 사용)
                optimizer.step()
                # optimizer.zero_grad(): 그래디언트 초기화 (PyTorch 옵티마이저 사용)
                optimizer.zero_grad()

                # @gpu_mem_guard: GPU 메모리 사용량 감시 및 조건부 실행 선언
                @gpu_mem_guard(threshold="80%")
                # 조건부 모델 저장 (선언적 조건문)
                if loss < config.save_threshold_loss:
                    # save_model: 모델 저장 Block/함수 호출 (PyTorch model.save 등 활용)
                    save_model(model, path=f"checkpoint_epoch_{epoch}.pth")

# Main Pipeline: 전체 LLM 빌드 과정을 연결하고 실행
pipeline llm_build_workflow:
    # DataPipeline Block 인스턴스 생성 및 설정
    let data_pipeline_instance = data_preparation.DataPipeline(text_files_pattern="my_corpus/*.txt")

    # Training Block 인스턴스 생성 및 설정, 의존성 연결 (requires를 통해 암시적 연결)
    # Training Block은 DataPipeline의 출력을 require하므로, minilang MetaVM이 실행 순서를 결정합니다.
    let training_instance = training.Training(epochs=200, learning_rate=1e-4)

    # Pipeline 간의 명시적 연결 (필요한 경우)
    # DataPipeline의 출력이 Training Block의 requires에 정의된 'data' 입력으로 연결됨은
    # requires 선언 자체로 충분하지만, 명시적으로 연결을 표현할 수도 있습니다.
    # connect(data_pipeline_instance.outputs.tokenized_batches, training_instance.requires.data)

    # 추가적인 실행 설정 (예: 모니터링)
    # ~~>: 모니터링 연결 선언
    ~~> RealTimePlotter(metrics=["loss", "perplexity"]) # 손실, 퍼플렉시티 실시간 플롯 Block 연결

# 이 DSL 정의는 minilang MetaVM에 의해 파싱되고,
# 실제 Python 코드 (NumPy, Pandas, PyTorch, cuTILING 호출 포함)로 변환되어 실행됩니다.
# MetaVM은 @parallel, @execution_context 설정을 기반으로 실행 환경을 관리합니다.
